<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>python爬虫小记 - 帘外五更风</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="wtnyzhsq"><meta name=description content="以今日头条为例通过分析 Ajax 抓取网页数据，并把数据分文件夹保存到本地。
"><meta name=keywords content="爬虫,python"><meta name=generator content="Hugo 0.82.0 with theme even"><link rel=canonical href=https://wtnyzhsq.xyz/post/spider/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/sass/main.min.39a3e01cac9473be1356f3572fcfe34b2e363efabad244a99a40f28a812c837e.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="python爬虫小记"><meta property="og:description" content="以今日头条为例通过分析 Ajax 抓取网页数据，并把数据分文件夹保存到本地。"><meta property="og:type" content="article"><meta property="og:url" content="https://wtnyzhsq.xyz/post/spider/"><meta property="article:section" content="post"><meta property="article:published_time" content="2021-03-02T18:50:50+08:00"><meta property="article:modified_time" content="2021-03-02T18:50:50+08:00"><meta itemprop=name content="python爬虫小记"><meta itemprop=description content="以今日头条为例通过分析 Ajax 抓取网页数据，并把数据分文件夹保存到本地。"><meta itemprop=datePublished content="2021-03-02T18:50:50+08:00"><meta itemprop=dateModified content="2021-03-02T18:50:50+08:00"><meta itemprop=wordCount content="826"><meta itemprop=keywords content="爬虫,python,"><meta name=twitter:card content="summary"><meta name=twitter:title content="python爬虫小记"><meta name=twitter:description content="以今日头条为例通过分析 Ajax 抓取网页数据，并把数据分文件夹保存到本地。"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Gopher</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>主页</li></a><a href=/post/><li class=mobile-menu-item>归档</li></a><a href=/tags/><li class=mobile-menu-item>标签</li></a><a href=/categories/><li class=mobile-menu-item>分类</li></a><a href=/about/><li class=mobile-menu-item>关于</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>Gopher</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>主页</a></li><li class=menu-item><a class=menu-item-link href=/post/>归档</a></li><li class=menu-item><a class=menu-item-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-item-link href=/categories/>分类</a></li><li class=menu-item><a class=menu-item-link href=/about/>关于</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>python爬虫小记</h1><div class=post-meta><span class=post-time>2021-03-02</span><div class=post-category><a href=/categories/%E7%88%AC%E8%99%AB/>爬虫</a></div><span class=more-meta>约 826 字</span>
<span class=more-meta>预计阅读 2 分钟</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><ul><li><ul><li><a href=#首先踩点>首先踩点</a></li></ul></li></ul></li></ul></nav></div></div><div class=post-content><p>以今日头条为例通过分析 Ajax 抓取网页数据，并把数据分文件夹保存到本地。</p><h3 id=首先踩点>首先踩点</h3><h4 id=-----分析内容----->- - -分析内容- - -</h4><p>搜索框输入‘街拍’，刷新后出现ajax请求，类型为XHR的即为ajax请求。点击preview分析内容，我们发现ajax请求的主要内容都在data标签里面，点开data，发现该标签里面分了很多小板块（0，1，2.等等），对比网页就可以得出这些小板块就是网页的一个模块，里面存放的东西就是我们要找的图片和标题。
<img src=https://cdn.jsdelivr.net/gh/wtnyzhsq/cdnstatic/img/%E8%B8%A9%E7%82%B92.png alt=踩点2 style=zoom:50%></p><p><img src=https://cdn.jsdelivr.net/gh/wtnyzhsq/cdnstatic/img/%E8%B8%A9%E7%82%B94.png alt=踩点3 style=zoom:50%></p><p><img src=https://cdn.jsdelivr.net/gh/wtnyzhsq/cdnstatic/img/%E8%B8%A9%E7%82%B95.png alt=踩点4 style=zoom:67%></p><h5 id=-----分析请求头----->- - -分析请求头- - -</h5><p>在这个网页里面，每一次加载就是一个ajax请求。这时候分析两三个请求头就可以发现，每次发送ajax请求url变化的只是offset和timestamp，复制RequestURL打开发现timestamp不用输也能得出结果。所以变化的相当与只有offset，每次变化都是20，说明每次加载20个模块，也就是20组图片。所以在这里设置一个循环就可以采集到很多的目标数据。</p><p><img src=https://cdn.jsdelivr.net/gh/wtnyzhsq/cdnstatic/img/%E8%B8%A9%E7%82%B95-1.png alt=踩点5 style=zoom:67%></p><p><img src=https://cdn.jsdelivr.net/gh/wtnyzhsq/cdnstatic/img/%E8%B8%A9%E7%82%B96.png alt=踩点6></p><h4 id=代码部分>代码部分</h4><h5 id=-----获取网页内容----->- - -获取网页内容- - -</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-bash data-lang=bash>import urllib.error
import requests
from urllib.parse import urlencode

def get_page<span class=o>(</span>Num_page<span class=o>)</span>:
    <span class=c1># 获取网页</span>
    <span class=nv>params</span> <span class=o>=</span> <span class=o>{</span>
        <span class=s1>&#39;aid&#39;</span>: <span class=s1>&#39;24&#39;</span>,
        <span class=s1>&#39;app_name&#39;</span>: <span class=s1>&#39;web_search&#39;</span>,
        <span class=s1>&#39;offset&#39;</span>: Num_page,
        <span class=s1>&#39;format&#39;</span>: <span class=s1>&#39;json&#39;</span>,
        <span class=s1>&#39;keyword&#39;</span>: <span class=s1>&#39;街拍&#39;</span>,
        <span class=s1>&#39;autoload&#39;</span>: <span class=s1>&#39;true&#39;</span>,
        <span class=s1>&#39;count&#39;</span>: <span class=s1>&#39;20&#39;</span>,
        <span class=s1>&#39;en_qc&#39;</span>: <span class=s1>&#39;1&#39;</span>,
        <span class=s1>&#39;cur_tab&#39;</span>: <span class=s1>&#39;1&#39;</span>,
        <span class=s1>&#39;from&#39;</span>: <span class=s1>&#39;search_tab&#39;</span>,
        <span class=s1>&#39;pd&#39;</span>: <span class=s1>&#39;synthesis&#39;</span>,
    <span class=o>}</span>
    <span class=nv>url</span> <span class=o>=</span> ex_url + urlencode<span class=o>(</span>params<span class=o>)</span>
    <span class=c1># urlencode方法用法可百度就能看懂</span>
    try:
        <span class=nv>response</span> <span class=o>=</span> requests.get<span class=o>(</span>url, <span class=nv>headers</span><span class=o>=</span>headers<span class=o>)</span>
        <span class=k>return</span> response.json<span class=o>()</span>
    except urllib.error.HTTPError as e:
        print<span class=o>(</span>e.reason<span class=o>)</span>
    except urllib.error.URLError as e:
        print<span class=o>(</span><span class=s2>&#34;Can&#39;t not connect url&#34;</span><span class=o>)</span>
    <span class=c1># 异常处理</span>
</code></pre></td></tr></table></div></div><h4 id=-----解析保存数据----->- - -解析保存数据- - -</h4><p>这里我用一个函数一起写了读取和保存操作,读取和保存独立分开比较好.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-bash data-lang=bash>from pyquery import PyQuery as pq
import os
import re

def get_img_url<span class=o>(</span>json<span class=o>)</span>:
    <span class=nv>items</span> <span class=o>=</span> json.get<span class=o>(</span><span class=s1>&#39;data&#39;</span><span class=o>)</span>
    <span class=c1># 调用get方法获取data标签内容</span>
    <span class=k>for</span> item in items:
        <span class=k>if</span> item.get<span class=o>(</span><span class=s1>&#39;emphasized&#39;</span><span class=o>)</span> is not None:
            <span class=nv>file_text</span> <span class=o>=</span> pq<span class=o>(</span>item.get<span class=o>(</span><span class=s1>&#39;emphasized&#39;</span><span class=o>)</span>.get<span class=o>(</span><span class=s1>&#39;title&#39;</span><span class=o>))</span>.text<span class=o>()</span>
            <span class=c1># 获取标题信息</span>
            <span class=nv>file_list</span> <span class=o>=</span> re.findall<span class=o>(</span>r<span class=s2>&#34;[\u4E00-\u9FFF]&#34;</span>, file_text, re.S<span class=o>)</span>
            <span class=c1># 正则匹配file_text中的汉字</span>
            <span class=nv>file_name</span> <span class=o>=</span> <span class=s1>&#39;&#39;</span>.join<span class=o>(</span>file_list<span class=o>)</span>
            print<span class=o>(</span>file_name<span class=o>)</span>
            os.mkdir<span class=o>(</span>file_name<span class=o>)</span>
            <span class=c1># 创建以file_name为名的文件夹</span>
            <span class=nv>lists</span> <span class=o>=</span> item.get<span class=o>(</span><span class=s1>&#39;image_list&#39;</span><span class=o>)</span>
            <span class=c1># 一组获取图片地址</span>
            <span class=k>for</span> lis in lists:
                try:
                    <span class=nv>response</span> <span class=o>=</span> requests.get<span class=o>(</span>lis<span class=o>[</span><span class=s1>&#39;url&#39;</span><span class=o>])</span>
                    <span class=nv>f_name</span> <span class=o>=</span> file_name + <span class=s1>&#39;/&#39;</span> + lis<span class=o>[</span><span class=s1>&#39;url&#39;</span><span class=o>]</span>.split<span class=o>(</span><span class=s1>&#39;/&#39;</span><span class=o>)[</span>-1<span class=o>]</span> + <span class=s1>&#39;.jpg&#39;</span>
                    with open<span class=o>(</span>f_name, <span class=s1>&#39;wb&#39;</span><span class=o>)</span> as f:
                        f.write<span class=o>(</span>response.content<span class=o>)</span>
                    <span class=c1># 请求一组图片里各个图片.并保存.</span>
                except urllib.error.HTTPError as e:
                    print<span class=o>(</span>e.reason<span class=o>)</span>
                except urllib.error.URLError as e:
                    print<span class=o>(</span><span class=s2>&#34;Can&#39;t not connect url&#34;</span><span class=o>)</span>
                <span class=c1># 异常处理</span>
</code></pre></td></tr></table></div></div><h4 id=-----函数运行----->- - -函数运行- - -</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-bash data-lang=bash><span class=k>if</span> <span class=nv>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span>:
    <span class=k>for</span> page in range<span class=o>(</span>0, 100, 20<span class=o>)</span>:
        <span class=nv>json</span> <span class=o>=</span> get_page<span class=o>(</span>page<span class=o>)</span>
        get_img_url<span class=o>(</span>json<span class=o>)</span>
</code></pre></td></tr></table></div></div><h4 id=以下为部分运行结果>以下为部分运行结果</h4><p><img src=https://cdn.jsdelivr.net/gh/wtnyzhsq/cdnstatic/img/%E7%BB%93%E6%9E%9C2.png alt=结果1 style=zoom:50%></p></div><footer class=post-footer><div class=post-tags><a href=/tags/%E7%88%AC%E8%99%AB.html>爬虫</a>
<a href=/tags/python.html>python</a></div><nav class=post-nav><a class=prev href=/post/sqlinjection/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">SQL宽字节注入</span>
<span class="prev-text nav-mobile">上一篇</span></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:wtnyzhsq@outlook.com class="iconfont icon-email" title=email></a><a href=https://github.com/wtnyzhsq class="iconfont icon-github" title=github></a><a href=https://wtnyzhsq.xyz/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>由 <a class=hexo-link href=https://gohugo.io>Hugo</a> 强力驱动</span>
<span class=division>|</span>
<span class=theme-info>主题 -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span>
<span class=copyright-year>&copy;
2020 -
2021<span class=heart><i class="iconfont icon-heart"></i></span><span>wtnyzhsq</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js></script><script id=baidu_push>(function(){var a,c,b;if(window.location.hostname==='localhost')return;a=document.createElement('script'),a.async=!0,c=window.location.protocol.split(':')[0],c==='https'?a.src='https://zz.bdstatic.com/linksubmit/push.js':a.src='http://push.zhanzhang.baidu.com/push.js',b=document.getElementsByTagName("script")[0],b.parentNode.insertBefore(a,b)})()</script></body></html>